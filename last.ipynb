{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Pandas, numpy, for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#\n",
    "import time\n",
    "# csv to write predict file\n",
    "import csv\n",
    "# open cv, scipy ndimage for image processing\n",
    "import cv2\n",
    "import random\n",
    "from scipy import ndarray\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "from scipy import ndimage\n",
    "#sklearn to train models\n",
    "from sklearn import preprocessing,cross_validation,neighbors\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and shapes printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features shape (10000, 2)\n",
      "testing  features shape (10000, 2)\n",
      "training labels shape (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "#training features\n",
    "images_train = np.load('all/train_images.npy', encoding='latin1')\n",
    "print(\"training features shape\",images_train.shape)\n",
    "\n",
    "#testing features\n",
    "images_test = np.load('all/test_images.npy', encoding='latin1')\n",
    "print(\"testing  features shape\",images_test.shape)\n",
    "\n",
    "#labels\n",
    "df = pd.read_csv('all/train_labels.csv')\n",
    "print(\"training labels shape\",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features new shape (10000, 10000)\n",
      "testing  features new shape (10000, 10000)\n",
      "training labels   new shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "#training data reshaping\n",
    "X=np.concatenate(images_train[:,1],axis=0)\n",
    "X=X.reshape(10000,10000)\n",
    "print(\"training features new shape\",X.shape)\n",
    "\n",
    "#testing data reshaping\n",
    "X_test1=np.concatenate(images_test[:,1],axis=0)\n",
    "X_test1=X_test1.reshape(10000,10000)\n",
    "print(\"testing  features new shape\",X_test1.shape)\n",
    "\n",
    "#training labels reshaping\n",
    "\n",
    "y=np.array(df.iloc[:,1])\n",
    "print(\"training labels   new shape\",y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for missing values and filling them with Median ( if there's missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Missing values) training features:  0\n",
      "(Missing values) testing  features:   0\n"
     ]
    }
   ],
   "source": [
    "# median imputer object \n",
    "imp = Imputer(strategy='median')\n",
    "\n",
    "# fit on the training data\n",
    "imp.fit(X)\n",
    "\n",
    "# Transform training and testing data\n",
    "X = imp.transform(X)\n",
    "X_test1 = imp.transform(X_test1)\n",
    "\n",
    "print('(Missing values) training features: ', np.sum(np.isnan(X)))\n",
    "print('(Missing values) testing  features:  ', np.sum(np.isnan(X_test1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure there is no infinite values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(~np.isfinite(X)))\n",
    "print(np.where(~np.isfinite(X_test1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerimg(im):\n",
    "    \"\"\"\n",
    "    takes a filtered image as argument and determins \n",
    "    mass center, then translates the image data \n",
    "    returns: centred image containing only relevant information \n",
    "    relevent pixels\n",
    "    \"\"\"\n",
    "    # Determine Centre of Mass\n",
    "    com = ndimage.measurements.center_of_mass(im)\n",
    "    # Translation distances in x and y axis\n",
    "    x_trans = int(im.shape[0]//2-com[0])\n",
    "    y_trans = int(im.shape[1]//2-com[1])\n",
    "\n",
    "    if x_trans > 0:\n",
    "        im2 = np.pad(im, ((x_trans, 0), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        im2 = np.pad(im, ((0, -x_trans), (0, 0)), mode='constant')\n",
    "        im2 = im2[-x_trans:, :]\n",
    "\n",
    "    if y_trans > 0:\n",
    "        im3 = np.pad(im2, ((0, 0), (y_trans, 0)), mode='constant')\n",
    "    else:\n",
    "        im3 = np.pad(im2, ((0, 0), (0, -y_trans)), mode='constant')\n",
    "        im3 = im3[:, -y_trans:]\n",
    "    im3=im3[35:75,35:75].copy()\n",
    "    return im3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering & resizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterimg(matrix):\n",
    "    \"\"\"\n",
    "    takes a 10000x10000 matrix reshapes every line  into (100 x 100 matrix)\n",
    "    denoises and resizes it  into a (40 x 40) matrix (without loosing information)\n",
    "    reshapes the input matrix into a (10000 x 1600)\n",
    "    \"\"\"\n",
    "    X_new=[]\n",
    "    for i in range(0,10000):\n",
    "        square = matrix[i].reshape(100,100)\n",
    "        square=np.int8(square)\n",
    "        image = square.astype('uint8')\n",
    "        nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=4)\n",
    "        sizes = stats[:, -1]\n",
    "        max_label = 1\n",
    "        max_size = sizes[1]\n",
    "        for i in range(2, nb_components):\n",
    "            if sizes[i] > max_size:\n",
    "                max_label = i\n",
    "                max_size = sizes[i]\n",
    "        img2 = np.zeros(output.shape)\n",
    "        img2[output == max_label] = 255\n",
    "        img2=centerimg(img2)\n",
    "        img2=img2.reshape(-1,1600)\n",
    "        X_new.append(img2)\n",
    "    X_new=np.array(X_new).reshape(len(X_new),-1)\n",
    "    return(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising , Resizing and translating Taining and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After pre processing] training features new shape (10000, 1600)\n",
      "[After pre processing] testing  features new shape (10000, 1600)\n"
     ]
    }
   ],
   "source": [
    "X_new=filterimg(X)\n",
    "print(\"[After pre processing] training features new shape\",X_new.shape)\n",
    "\n",
    "X_test1_new=filterimg(X_test1)\n",
    "print(\"[After pre processing] testing  features new shape\",X_test1_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_rotation(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 25% on the left and 25% on the right\n",
    "    random_degree = random.uniform(-25, 25)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    new_image=random_rotation(X_new[i].reshape(40,40))\n",
    "    new_image=new_image.reshape(1,1600)\n",
    "    X_new=np.concatenate([X_new, new_image])\n",
    "   \n",
    "    y=np.hstack([y,y[i] ])\n",
    "   \n",
    "    \n",
    "    \n",
    "y.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train (80%) & test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_new, y, test_size=split,random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shapes after spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8800, 1600)\n",
      "(2200, 1600)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Clean data ( 3 training Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,15):\n",
    "    trainsetmtx = np.reshape(X_train[i], (40,40))\n",
    "    print(trainsetmtx.shape)\n",
    "    imgplot = plt.imshow(trainsetmtx)\n",
    "    print ('Label1 = %s' % y_train[i]), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning images used for prediction: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Clean data ( 3 testing Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,15):\n",
    "    trainsetmtx = np.reshape(X_test1_new[i], (40,40))\n",
    "    imgplot = plt.imshow(trainsetmtx)\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with test sampling\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.22318181818181818 \n"
     ]
    }
   ],
   "source": [
    "#print accuracy\n",
    "print(\"Accuracy:{} \".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagged decision trees\n",
    "After data preparation, we're going to use Bagged Decision Trees.\n",
    "We are going to test the accuracy with differents number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1754.9418s\n",
      "0.5169318181818182\n",
      "Time: 4128.1049s\n",
      "0.52625\n"
     ]
    }
   ],
   "source": [
    "seed = 8\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "clf = DecisionTreeClassifier()\n",
    "ntrees=[100,150,200,250,280,290,300]\n",
    "result=[]\n",
    "t0 = time.clock()\n",
    "for i in ntrees:\n",
    "    model1 = BaggingClassifier(base_estimator=clf, n_estimators=i, random_state=8)\n",
    "    results = model_selection.cross_val_score(model1, X_train, y_train, cv=kfold)\n",
    "    print (\"Time: %.4fs\" % (time.clock()-t0))\n",
    "    print(results.mean())\n",
    "    result.append(results.mean())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate plot\n",
    "\n",
    "%matplotlib inline\n",
    "plt.title('Bagged Decision Trees')\n",
    "plt.plot(ntrees, result, label='Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('num_trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "\n",
    "seed = 7\n",
    "t1 = time.clock()\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "clf = DecisionTreeClassifier()\n",
    "n_trees = 300\n",
    "model1 = BaggingClassifier(base_estimator=clf, n_estimators=n_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model1, X_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Time: %.4fs\" % (time.clock()-t1))\n",
    "print(results.mean())\n",
    "cft1=model1.fit(X_train,y_train)\n",
    "y_pred1=cft1.predict(X_test1_new)\n",
    "print(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final9.csv', 'w',newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(['Id','Category'])\n",
    "    for i in range(len(y_pred1)):\n",
    "         spamwriter.writerow([i, y_pred1[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
